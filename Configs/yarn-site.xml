<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <property>
    <name>yarn.resourcemanager.resource-tracker.address</name>
    <value>10.0.2.15:8031</value>
    <description>host is the hostname of the resource manager and port is the port on which the NodeManagers contact the Resource Manager.</description>
  </property>
   <property>
    <name>yarn.resourcemanager.resource-tracker.port</name>
    <value>8031</value>
  </property>
  <property>
    <name>yarn.resourcemanager.scheduler.address</name>
    <value>10.0.2.15:8030</value>
  </property>
  <property>
    <name>yarn.resourcemanager.scheduler.port</name>
    <value>8030</value>
  </property>
  <property>
    <name>yarn.resourcemanager.address</name>
    <value>10.0.2.15:8032</value>
  </property>
  <property>
    <name>yarn.resourcemanager.port</name>
    <value>8032</value>
  </property>
  <property>
    <name>yarn.resourcemanager.admin.address</name>
    <value>10.0.2.15:8033</value>
  </property>
  <property>
    <name>yarn.resourcemanager.admin.port</name>
    <value>8033</value>
  </property>
  <property>
    <name>yarn.resourcemanager.groupMembership.address</name>
    <value>10.0.2.15:8034</value>
  </property>
  <property>
    <name>yarn.resourcemanager.groupMembership.port</name>
    <value>8034</value>
  </property>

  <property>
    <name>yarn.resourcemanager.webapp.address</name>
    <value>0.0.0.0:8088</value>
    <description>The http address of the RM web application.</description>
  </property>

  <property>
    <name>yarn.nodemanager.delete.debug-delay-sec</name>
    <value>0</value>
  </property>
  <property>
    <description>NM Webapp address.</description>
    <name>yarn.nodemanager.webapp.address</name>
    <value>10.0.2.15:8042</value>
  </property>
  <property>
    <name>yarn.nodemanager.address</name>
    <value>10.0.2.15:9000</value>
  </property>
  <property>
    <name>yarn.nodemanager.localizer.address</name>
    <value>10.0.2.15:9001</value>
  </property>
  <property>
    <name>yarn.application.classpath</name>
    <value>/srv/hops/hadoop-2.7.3, 
                                                  /srv/hops/hadoop-2.7.3/lib/*, 
                                                  /srv/hops/hadoop-2.7.3/etc/hadoop/,  
                                                  /srv/hops/hadoop-2.7.3/share/hadoop/common/*, 
                                                  /srv/hops/hadoop-2.7.3/share/hadoop/common/lib/*, 
                                                  /srv/hops/hadoop-2.7.3/share/hadoop/hdfs/*, 
                                                  /srv/hops/hadoop-2.7.3/share/hadoop/hdfs/lib/*, 
                                                  /srv/hops/hadoop-2.7.3/share/hadoop/yarn/*, 
                                                  /srv/hops/hadoop-2.7.3/share/hadoop/yarn/lib/*, 
                                                  /srv/hops/hadoop-2.7.3/share/hadoop/tools/lib/*, 
                                                  /srv/hops/hadoop-2.7.3/share/hadoop/mapreduce/*, 
                                                  /srv/hops/hadoop-2.7.3/share/hadoop/mapreduce/lib/*</value>
  </property>
  <property>
    <name>yarn.resourcemanager.scheduler.class</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
    <description>In case you do not want to use the default scheduler</description>
  </property>

  <property>
    <name>yarn.nodemanager.container-executor.class</name>
    <value>org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor</value>
    <description>LinuxContainerExecutor uses CGroups, DefaultContainerExecutor uses Unix processes.</description>
  </property>
  <property>
    <name>yarn.nodemanager.linux-container-executor.group</name>
    <value>hadoop</value>
  </property>
<property>
  <name>yarn.nodemanager.container-executor.resources-handler.class</name>
  <value>org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler</value>
</property>
  <property>
    <name>yarn.nodemanager.linux-container-executor.cgroups.hierarchy</name>
    <value>/yarn</value>
    <description>LinuxContainerExecutor CGroups hierarchy.</description>
  </property>
  <property>
    <name>yarn.nodemanager.linux-container-executor.cgroups.mount-path</name>
    <value>/cgroup</value>
    <description>LinuxContainerExecutor CGroups mount-path.</description>
  </property>
  <property>
    <name>yarn.nodemanager.linux-container-executor.cgroups.mount</name>
    <value>true</value>
    <description>Whether the LCE should attempt to mount cgroups if not found. Only used when the LCE resources handler is set to the CgroupsLCEResourcesHandler.</description>
  </property>
<property>
  <name>yarn.nodemanager.resource.percentage-physical-cpu-limit</name>
  <value>100</value>
</property>
<property>
  <name>yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage</name>
  <value>false</value>
</property>
  <property>
    <name>yarn.resourcemanager.scheduler.client.thread-count</name>
    <value>50</value>
    <description>Number of ResourceManager threads for decoding and handling client RPCs for apps.</description>
  </property>
  <property>
    <name>yarn.resourcemanager.resource-tracker.client.thread-count</name>
    <value>50</value>
    <description>Number of ResourceTracker threads for decoding and handling client RPCs for NodeManager heartbeats.</description>
  </property>
  <property>
    <name>yarn.resourcemanager.admin.client.thread-count</name>
    <value>1</value>
    <description>Number of ResourceManager threads for decoding and handling client RPCs for Admin operations.</description>
  </property>
  <property>
    <name>yarn.nodemanager.container-manager.thread-count</name>
    <value>20</value>
    <description>Number of threads for starting/stopping containers</description>
  </property>
  <property>
    <name>yarn.nodemanager.local-dirs</name>
    <value>/srv/hops/hadoop/tmp/nm-local-dir</value>
    <description>the local directories used by the nodemanager to store its localized files</description>
  </property>

  <property>
    <name>yarn.nodemanager.log-dirs</name>
    <value>/srv/hops/hadoop/logs/userlogs</value>
    <description>the local directories used by the nodemanager to store its localized logs</description>
  </property>
  <property>
    <name>yarn.nodemanager.remote-app-log-dir</name>
    <value>/user/glassfish/logs</value>
    <description>the directory where the logs are aggregated</description>
  </property>

  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>spark_shuffle,mapreduce_shuffle</value>
    <description>shuffle service that needs to be set for Spark external shuffle service (dynamic allocators) and  MapReduce to run</description>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services.spark_shuffle.class</name>
    <value>org.apache.spark.network.yarn.YarnShuffleService</value>
    <description>The exact name of the class for the Spark external shuffle service</description>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    <description>The exact name of the class for shuffle service</description>
  </property>
  
  <property>
    <name>yarn.nodemanager.vmem-pmem-ratio</name>
    <value>4.1</value>
    <description>The virtual memory (physical + paged memory) upper limit for each Map and Reduce task is determined by the virtual memory ratio each YARN Container is allowed.</description>
  </property>
  <property>
    <name>yarn.nodemanager.vmem-check-enabled</name>
    <value>false</value>
    <description></description>
  </property>
  <property>
    <name>yarn.nodemanager.pmem-check-enabled</name>
    <value>true</value>
    <description></description>
  </property>
  <property>
    <name>yarn.nodemanager.resource.memory-mb</name>
    <value>11000</value>
    <description>the amount of memory on the NodeManager in MB</description>
  </property>
  <property>
    <name>yarn.scheduler.minimum-allocation-mb</name>
    <value>128</value>
  </property>
  <property>
    <name>yarn.scheduler.maximum-allocation-mb</name>
    <value>2500</value>
    <description>The maximum allocation for every container request at the RM, in MBs. Memory requests higher than this won't take effect, and will get capped to this value.</description>
  </property>
  <property>
    <name>yarn.nodemanager.resource.cpu-vcores</name>
    <value>4</value>
    <description>Number of CPU cores that can be allocated for containers.</description>
  </property>
  <property>
    <name>yarn.scheduler.minimum-allocation-vcores</name>
    <value>1</value>
    <description>The minimum allocation for every container request at the RM, in terms of virtual CPU cores. Requests lower than this won't take effect, and the specified value will get allocated the minimum.</description>
  </property>
  <property>
    <name>yarn.scheduler.maximum-allocation-vcores</name>
    <value>4</value>
    <description>The maximum allocation for every container request at the RM, in terms of virtual CPU cores. Requests higher than this won't take effect, and will get capped to this value.</description>
  </property>
  <property>
    <name>yarn.nodemanager.log.retain-seconds</name>
    <value>86400</value>
    <description>Default time (in seconds) to retain log files on the NodeManager Only applicable if log-aggregation is disabled.</description>
  </property>
  <property>
    <name>yarn.resourcemanager.am.max-retries</name>
    <value>2</value>
    <description>Number of times to try to restart the ApplicationMaster by the ResourceManager if its NodeManager fails.</description>
  </property>
  <property>
    <name>yarn.log-aggregation-enable</name>
    <value>true</value>
    <description>Enable Default aggregatioin of log files HDFS, deleting them from the NodeManager.</description>
  </property>
  <property>
    <name>yarn.log-aggregation.retain-seconds</name>
    <value>86400</value>
    <description>Default time (in seconds) to retain log files in HDFS. Only applicable if log-aggregation is disabled.</description>
  </property>
  <property>
    <name>yarn.log-aggregation.retain-check-interval-seconds</name>
    <value>100</value>
    <description>Default time (in seconds) between checks for retained log files in HDFS. Only applicable if log-aggregation is disabled.</description>
  </property>
  <property>
    <name>yarn.nodemanager.heartbeat.interval-ms</name>
    <value>1000</value>
    <description>Time in ms between heartbeats sent from the NodeManager to the ResourceManager.</description>
  </property>

  <!--HA-->
  <property>
    <name>yarn.resourcemanager.ha.enabled</name>
    <value>false</value>
  </property>
 <property>
    <name>yarn.resourcemanager.ha.automatic-failover.enabled</name>
    <value>false</value>
  </property>
  <property>
    <name>yarn.resourcemanager.recovery.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.resourcemanager.ha.rm-ids</name>
    <value>10.0.2.15</value>
  </property>



  <property>
    <name>yarn.resourcemanager.hostname.10.0.2.15</name>
    <value>10.0.2.15</value>
  </property>
  <property>
    <name>yarn.resourcemanager.scheduler.address.10.0.2.15</name>
    <value>10.0.2.15:8030</value>
  </property>
  <property>
    <name>yarn.resourcemanager.resource-tracker.address.10.0.2.15</name>
    <value>10.0.2.15:8031</value>
  </property>
  <property>
    <name>yarn.resourcemanager.address.10.0.2.15</name>
    <value>10.0.2.15:8032</value>
  </property>
  <property>
    <name>yarn.resourcemanager.admin.address.10.0.2.15</name>
    <value>10.0.2.15:8033</value>
  </property>
  <property>
    <name>yarn.resourcemanager.groupMembership.address.10.0.2.15</name>
    <value>10.0.2.15:8034</value>
  </property>
  <property>
    <name>yarn.resourcemanager.webapp.address.10.0.2.15</name>
    <value>10.0.2.15:8088</value>
  </property>


  <property>
    <name>yarn.resourcemanager.store.class</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.DBRMStateStore</value>
  </property>
  <property>
    <name>dfs.leader.check.interval</name>
    <value>1000</value>
  </property>

  <!--ndb-->
  <!--batching rpcs for commiting to db-->
  <property>
      <name>hops.yarn.resourcemanager.batch.max.size</name><!-- uber pricing -->
      <value>60</value>
  </property>
  <property>
      <name>hops.yarn.resourcemanager.batch.max.duration</name>
      <value>60</value>
  </property>
  <!--distributed mode-->
  <property>
    <name>yarn.client.failover-distributed</name>
    <value>false</value>
  </property>
  
  <property>
      <name>hops.yarn.resourcemanager.ndb-event-streaming.enable</name>
      <value>true</value>
  </property>

  <property>
    <name>yarn.client.failover-sleep-base-ms</name>
    <value>100</value>
  </property>

  <property>
    <name>yarn.client.failover-sleep-max-ms</name>
    <value>1000</value>
  </property>

  <!-- quotas -->
  <property>
    <name>yarn.quotas.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.quotas.containers-logs.monitor-interval</name>
    <value>1000</value>
  </property>
  <property>
    <name>yarn.quotas.ticks.per.credit</name>
    <value>60</value>
  </property>
  <property>
    <name>yarn.quotas.ticks.per.credit</name>
    <value>600</value>
  </property>
  <property>
    <name>yarn.quotas.containers-logs.checkpoints-minticks</name>
    <value>600</value>
  </property>
  <property> 
    <name>yarn.resourcemanager.proxy-user-privileges.enabled</name>
    <value>true</value>
  </property>

<!-- uber pricing -->
<property>
    <name>yarn.quotas.overpricing-threshold.mb</name>
    <value>0.1</value>
</property>

<property>
    <name>yarn.quotas.overpricing-threshold.mb</name>
    <value>0.1</value>
</property>

<!-- container monitoring and metrics period -->
  <property>
    <name>yarn.nodemanager.resource-monitor.interval-ms</name>
    <value>2000</value>
    <description>How often to monitor the node and the containers. If 0 or negative, monitoring is disabled.</description>
  </property>
  <property>
    <name>yarn.nodemanager.container-monitor.enabled</name>
    <value>false</value>
    <description>Enable container monitor.</description>
  </property>
  <property>
    <name>yarn.nodemanager.container-monitor.interval-ms</name>
    <value>1000</value>
    <description>How often to monitor containers.</description>
  </property>
  <property>
    <name>yarn.nodemanager.container-metrics.period-ms</name>
    <value>4000</value>
    <description>Container metrics flush period in ms. Set to -1 for flush on completion.</description>
  </property>
  
</configuration>


<!-- container monitoring and metrics period -->
  <property>
    <name>yarn.nodemanager.container-metrics.period-ms</name>
    <value>1000</value>
    <description>Container metrics flush period in ms. Set to -1 for flush on completion.</description>
  </property>

